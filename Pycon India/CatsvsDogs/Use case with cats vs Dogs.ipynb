{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into the code directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'example-versioning' already exists and is not an empty directory.\n",
      "/home/kurian/Documents/Talks/Pycon India/CatsvsDogs/example-versioning\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/iterative/example-versioning.git\n",
    "%cd example-versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  requirements.txt  train.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow>=1.11.0\r\n",
      "keras==2.2.4\r\n",
      "pillow==5.3.0\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow>=1.11.0 (from -r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting keras==2.2.4 (from -r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Collecting pillow==5.3.0 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/8c/230204b8e968f6db00c765624f51cfd1ecb6aea57b25ba00b240ee3fb0bd/Pillow-5.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 166kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages (from tensorflow>=1.11.0->-r requirements.txt (line 1)) (0.33.4)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting absl-py>=0.7.0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/0d/7cbf64cac3f93617a2b6b079c0182e4a83a3e7a8964d3b0cc3d9758ba002/absl-py-0.8.0.tar.gz (102kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 94kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/fd/60ce148d8e4205bdf6da4ffec31348fd33f710c20a882b44319d54fd51ae/protobuf-3.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 93kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages (from tensorflow>=1.11.0->-r requirements.txt (line 1)) (1.12.0)\n",
      "Collecting gast>=0.2.0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages (from tensorflow>=1.11.0->-r requirements.txt (line 1)) (1.16.4)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "Collecting grpcio>=1.8.6 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/27/1f908ebb99c8d48a5ba4eb9d7997f5633b920d98fe712f67aaa0663f1307/grpcio-1.23.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 67kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages (from keras==2.2.4->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting pyyaml (from keras==2.2.4->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 56kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting h5py (from keras==2.2.4->-r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 262kB/s eta 0:00:01     |██████████████████████████▌     | 2.4MB 278kB/s eta 0:00:02\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 91kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.11.0->-r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.11.0->-r requirements.txt (line 1)) (41.0.1)\n",
      "Building wheels for collected packages: absl-py, gast, pyyaml\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.8.0-cp37-none-any.whl size=120986 sha256=eb5f1b29828e4d2133a0076cdf93cfa08249d736d6235641cbbdc92af090fba9\n",
      "  Stored in directory: /home/kurian/.cache/pip/wheels/9a/1e/7a/456008eb5e47fd5de792c6139df6d5b3d5f71d51c6a0b94799\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.3.2-cp37-none-any.whl size=9678 sha256=df89570a938fe8d7ead357998287c3fdb1af9fb82c401f188f833ff5b9384e2a\n",
      "  Stored in directory: /home/kurian/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp37-cp37m-linux_x86_64.whl size=44104 sha256=5d54779c1acd71c160c1d2a564233639ca197dbe4491433bd0eef56fbe43974d\n",
      "  Stored in directory: /home/kurian/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
      "Successfully built absl-py gast pyyaml\n",
      "Installing collected packages: grpcio, werkzeug, absl-py, markdown, protobuf, tensorboard, tensorflow-estimator, astor, h5py, keras-applications, gast, termcolor, wrapt, keras-preprocessing, google-pasta, tensorflow, pyyaml, keras, pillow\n",
      "Successfully installed absl-py-0.8.0 astor-0.8.0 gast-0.3.2 google-pasta-0.1.7 grpcio-1.23.0 h5py-2.10.0 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 pillow-5.3.0 protobuf-3.9.1 pyyaml-5.1.2 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-0.16.0 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kurian/Documents/Talks/Pycon India/CatsvsDogs/example-versioning/data\n"
     ]
    }
   ],
   "source": [
    "! mkdir data\n",
    "%cd data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dvc get` can download data artifacts\n",
    "from any DVC project\n",
    "hosted on a Git repository into the current working directory (similar to wget but for DVC repositories). In this case we use our own iterative/dataset-registry) project as the external data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPreparing to download data from 'https://remote.dvc.org/dataset-registry'\n",
      "\u001b[KPreparing to collect status from https://remote.dvc.org/dataset-registry\n",
      "\u001b[KCollecting information from local cache...\n",
      "\u001b[K[##############################] 100% \n",
      "\n",
      "\u001b[KCollecting information from remote cache...\n",
      "\u001b[K[##############################] 100% https://remote.dvc.org/dataset-registry/fa/9c0eb4173d86695b4e800219651360\n",
      "\u001b[K[##############################] 100% Analysing status\n",
      "\u001b[K[##############################] 100% ../../../../../../../../tmp/tmp8im7l8x_dvc-repo/tutorial/ver/data.zip\n",
      "\u001b[K\u001b[31mERROR\u001b[39m: failed to download 'https://remote.dvc.org/dataset-registry/fa/9c0eb4173d86695b4e800219651360' to '.cfqWvCUbS75nmFgjm8Zka9/fa/9c0eb4173d86695b4e800219651360' - HTTPSConnectionPool(host='s3-us-east-2.amazonaws.com', port=443): Read timed out.\n",
      "\n",
      "\u001b[33mHaving any troubles?\u001b[39m. Hit us up at \u001b[34mhttps://dvc.org/support\u001b[39m, we are always happy to help!\n",
      "\n",
      "\u001b[K\u001b[31mERROR\u001b[39m: failed to get 'tutorial/ver/data.zip' from 'https://github.com/iterative/dataset-registry' - 1 files failed to download\n",
      "\n",
      "\u001b[33mHaving any troubles?\u001b[39m. Hit us up at \u001b[34mhttps://dvc.org/support\u001b[39m, we are always happy to help!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc get https://github.com/iterative/dataset-registry \\\n",
    "          tutorial/ver/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KSetting 'storage' as a default remote.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc remote add -d --local storage s3://dvc-public/remote/dataset-registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc remote list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[31mERROR\u001b[39m: failed to get 'new-labels.zip.dvc' from 'https://github.com/iterative/dataset-registry/tree/master/tutorial/ver' - Failed to clone repo 'https://github.com/iterative/dataset-registry/tree/master/tutorial/ver' to '/tmp/tmpg_6o3j4tdvc-repo': Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone --no-single-branch -v https://github.com/iterative/dataset-registry/tree/master/tutorial/ver /tmp/tmpg_6o3j4tdvc-repo\n",
      "  stderr: 'Cloning into '/tmp/tmpg_6o3j4tdvc-repo'...\n",
      "fatal: repository 'https://github.com/iterative/dataset-registry/tree/master/tutorial/ver/' not found\n",
      "'\n",
      "\n",
      "\u001b[33mHaving any troubles?\u001b[39m. Hit us up at \u001b[34mhttps://dvc.org/support\u001b[39m, we are always happy to help!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc get https://github.com/iterative/dataset-registry/tree/master/tutorial/ver  new-labels.zip.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPreparing to download data from 'https://remote.dvc.org/dataset-registry'\n",
      "\u001b[KPreparing to collect status from https://remote.dvc.org/dataset-registry\n",
      "\u001b[KCollecting information from local cache...\n",
      "\u001b[K[##############################] 100% \n",
      "\n",
      "\u001b[KCollecting information from remote cache...\n",
      "\u001b[K[##############################] 100% https://remote.dvc.org/dataset-registry/fa/9c0eb4173d86695b4e800219651360\n",
      "\u001b[K[##############################] 100% Analysing status\n",
      "\u001b[K[##############################] 100% ../../../../../../../../tmp/tmpnofw03ckdvc-repo/tutorial/ver/data.zip\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc get https://github.com/iterative/dataset-registry tutorial/ver/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kurian/Documents/Talks/Pycon India/CatsvsDogs/example-versioning\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2019-09-22 12:59:29.903368: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-09-22 12:59:29.923243: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz\n",
      "2019-09-22 12:59:29.923535: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f7da34d4d0 executing computations on platform Host. Devices:\n",
      "2019-09-22 12:59:29.923573: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-09-22 12:59:30.038473: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "2019-09-22 12:59:30.487427: W tensorflow/core/framework/allocator.cc:107] Allocation of 57600000 exceeds 10% of system memory.\n",
      "2019-09-22 12:59:30.534918: W tensorflow/core/framework/allocator.cc:107] Allocation of 57600000 exceeds 10% of system memory.\n",
      "2019-09-22 12:59:31.608644: W tensorflow/core/framework/allocator.cc:107] Allocation of 57600000 exceeds 10% of system memory.\n",
      "2019-09-22 12:59:31.637926: W tensorflow/core/framework/allocator.cc:107] Allocation of 57600000 exceeds 10% of system memory.\n",
      "2019-09-22 12:59:32.469339: W tensorflow/core/framework/allocator.cc:107] Allocation of 57600000 exceeds 10% of system memory.\n",
      "Found 800 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1000 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.7881 - acc: 0.7190 - val_loss: 0.2959 - val_acc: 0.8837\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3985 - acc: 0.8450 - val_loss: 0.2716 - val_acc: 0.8987\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3498 - acc: 0.8620 - val_loss: 0.3071 - val_acc: 0.8900\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2874 - acc: 0.8900 - val_loss: 0.5324 - val_acc: 0.8063\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2172 - acc: 0.9270 - val_loss: 0.2967 - val_acc: 0.9025\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2078 - acc: 0.9260 - val_loss: 0.4126 - val_acc: 0.8762\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1528 - acc: 0.9380 - val_loss: 0.3937 - val_acc: 0.8987\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1737 - acc: 0.9410 - val_loss: 0.3825 - val_acc: 0.8987\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1560 - acc: 0.9430 - val_loss: 0.3978 - val_acc: 0.8825\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1246 - acc: 0.9540 - val_loss: 0.5828 - val_acc: 0.8687\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.53.1\u001b[39m -> \u001b[32m0.59.2\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[KAdding 'model.h5' to '.gitignore'.\n",
      "\u001b[KSaving 'model.h5' to '.dvc/cache/a2/a066d83aff3a06cc191f185a7d6041'.\n",
      "\u001b[KSaving information to 'model.h5.dvc'.\n",
      "\u001b[K\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add .gitignore model.h5.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc add model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 2a77136] First model, trained with 1000 images\r\n",
      " 4 files changed, 19 insertions(+)\r\n",
      " create mode 100644 data.dvc\r\n",
      " create mode 100644 metrics.json\r\n",
      " create mode 100644 model.h5.dvc\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! git add .gitignore model.h5.dvc data.dvc metrics.json\n",
    "! git commit -m \"First model, trained with 1000 images\"\n",
    "! git tag -a \"v1.0\" -m \"model v1.0, 1000 images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model with more Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPreparing to download data from 'https://remote.dvc.org/dataset-registry'\n",
      "\u001b[KPreparing to collect status from https://remote.dvc.org/dataset-registry\n",
      "\u001b[KCollecting information from local cache...\n",
      "\u001b[K[##############################] 100% \n",
      "\n",
      "\u001b[KCollecting information from remote cache...\n",
      "\u001b[K[##############################] 100% https://remote.dvc.org/dataset-registry/2e/aa473159443e75e6fb7b29e56c0787\n",
      "\u001b[K[##############################] 100% Analysing status\n",
      "\u001b[K[##############################] 100% ../../../../../../../tmp/tmp7_nl_0v7dvc-repo/tutorial/ver/new-labels.zip[##############################] 100% ../../../../../../../tmp/tmp7_nl_0v7dvc-repo/tutorial/ver/new-labels.zip[##############################] 100% ../../../../../../../tmp/tmp7_nl_0v7dvc-repo/tutorial/ver/new-labels.zip\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc get https://github.com/iterative/dataset-registry \\\n",
    "          tutorial/ver/new-labels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open new-labels.zip, new-labels.zip.zip or new-labels.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "! unzip new-labels.zip\n",
    "! rm -f new-labels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.53.1\u001b[39m -> \u001b[32m0.59.2\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[K[##############################] 100% Created unpacked dir\n",
      "\n",
      "\u001b[KComputing md5 for a large number of files. This is only done once.\n",
      "\u001b[K[##############################] 100% \n",
      "\n",
      "\u001b[K\u001b[33mWARNING\u001b[39m: Output 'data' of 'data.dvc' changed because it is 'modified'\n",
      "\n",
      "\u001b[KSaving 'data' to '.dvc/cache/21/060888834f7220846d1c6f6c04e649.dir'.\n",
      "\n",
      "\u001b[KSaving information to 'data.dvc'.\n",
      "\n",
      "\u001b[K\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data.dvc\n",
      "\u001b[K\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.53.1\u001b[39m -> \u001b[32m0.59.2\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[0mUsing TensorFlow backend.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2019-09-22 13:11:29.959943: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-09-22 13:11:29.986970: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz\n",
      "2019-09-22 13:11:29.987621: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56156939f7f0 executing computations on platform Host. Devices:\n",
      "2019-09-22 13:11:29.987676: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-09-22 13:11:30.136349: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kurian/Learnings/mlcourse.ai/.env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6734 - acc: 0.7740 - val_loss: 0.4146 - val_acc: 0.8125\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3574 - acc: 0.8650 - val_loss: 0.2512 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3144 - acc: 0.8835 - val_loss: 0.2690 - val_acc: 0.8987\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2607 - acc: 0.9030 - val_loss: 0.2989 - val_acc: 0.8850\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2231 - acc: 0.9145 - val_loss: 0.5476 - val_acc: 0.8162\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2085 - acc: 0.9270 - val_loss: 0.3168 - val_acc: 0.8962\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1867 - acc: 0.9335 - val_loss: 0.3699 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1909 - acc: 0.9370 - val_loss: 0.3238 - val_acc: 0.8950\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1544 - acc: 0.9465 - val_loss: 0.4470 - val_acc: 0.8775\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1298 - acc: 0.9535 - val_loss: 0.5102 - val_acc: 0.8900\n",
      "\u001b[K\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m     Update available \u001b[31m0.53.1\u001b[39m -> \u001b[32m0.59.2\u001b[39m     \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m       Run \u001b[33mpip\u001b[39m install dvc \u001b[34m--upgrade\u001b[39m       \u001b[33m|\u001b[39m\n",
      "\u001b[33m|\u001b[39m                                           \u001b[33m|\u001b[39m\n",
      "\u001b[33m+-------------------------------------------+\n",
      "\u001b[39m\n",
      "\u001b[K\u001b[33mWARNING\u001b[39m: Output 'model.h5' of 'model.h5.dvc' changed because it is 'modified'\n",
      "\u001b[KSaving 'model.h5' to '.dvc/cache/f6/d0d7440df494f3128d41a6257dfb1f'.\n",
      "\u001b[KSaving information to 'model.h5.dvc'.\n",
      "\u001b[K\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add model.h5.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! dvc add data\n",
    "! dvc remove model.h5.dvc\n",
    "! python train.py\n",
    "! dvc add model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f91b4b7] Second model, trained with 2000 images\n",
      " 3 files changed, 5 insertions(+), 5 deletions(-)\n",
      " rewrite metrics.json (100%)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git add model.h5.dvc data.dvc metrics.json\n",
    "git commit -m \"Second model, trained with 2000 images\"\n",
    "git tag -a \"v2.0\" -m \"model v2.0, 2000 images\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%bash` not found (But cell magic `%%bash` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
